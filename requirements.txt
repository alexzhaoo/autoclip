# ⚠️ Torch + flash-attn must match for Wan2.2 to work
# This file reflects the versions tested in the container:
# PyTorch 2.8.0 with CUDA 12.8, FlashAttention 2.8.3
# Install PyTorch with CUDA 12.8 wheels via:
# pip install torch==2.8.0+cu128 torchvision==0.23.0+cu128 torchaudio==2.8.0+cu128 --index-url https://download.pytorch.org/whl/cu128
--extra-index-url https://download.pytorch.org/whl/cu128
torch==2.8.0+cu128
torchvision==0.23.0+cu128
torchaudio==2.8.0+cu128

# FlashAttention (install manually with build flags)
# Run: pip install flash-attn==2.8.3 --no-build-isolation --use-pep517
flash-attn==2.8.3

# Core dependencies
opencv-python>=4.9.0.80
diffusers>=0.31.0
transformers>=4.49.0,<=4.51.3
tokenizers>=0.20.3
accelerate>=1.1.1
tqdm
imageio[ffmpeg]
easydict
ftfy
dashscope
imageio-ffmpeg
numpy>=1.23.5,<2
openai>=1.0.0
python-dotenv>=1.0.0
yt-dlp>=2023.1.0

# Video processing
mediapipe>=0.10.0
moviepy>=1.0.3
pysubs2>=1.6.0
pysrt>=1.1.2
decord>=0.6.0

# Audio processing
faster-whisper>=0.9.0
pydub>=0.25.0
elevenlabs>=0.2.0

# Text processing
nltk>=3.8.0

# AI/ML for B-roll generation
transformers>=4.30.0
accelerate>=0.20.0
pillow>=10.0.0
diffusers[torch]>=0.18.0

# Web requests
requests>=2.31.0

# Vast.ai SDK
vastai-sdk>=0.1.0

# Additional utilities
pathlib2>=2.3.0
dataclasses>=0.6; python_version<"3.7"
decord
librosa
peft
